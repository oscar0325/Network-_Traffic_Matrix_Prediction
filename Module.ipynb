{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b21cbf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric_temporal.nn.recurrent import A3TGCN2\n",
    "import torch_geometric_temporal.nn.recurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77341afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yang/miniconda3/envs/pygt/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/attentiontemporalgcn.py'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric_temporal.nn.recurrent.attentiontemporalgcn.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25d76e",
   "metadata": {},
   "source": [
    "class A3TGCN2(torch.nn.Module):\n",
    "    r\"\"\"An implementation THAT SUPPORTS BATCHES of the Attention Temporal Graph Convolutional Cell.\n",
    "    For details see this paper: `\"A3T-GCN: Attention Temporal Graph Convolutional\n",
    "    Network for Traffic Forecasting.\" <https://arxiv.org/abs/2006.11583>`_\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        periods (int): Number of time periods.\n",
    "        improved (bool): Stronger self loops (default :obj:`False`).\n",
    "        cached (bool): Caching the message weights (default :obj:`False`).\n",
    "        add_self_loops (bool): Adding self-loops for smoothing (default :obj:`True`).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int, \n",
    "        out_channels: int,  \n",
    "        periods: int, \n",
    "        batch_size:int, \n",
    "        improved: bool = False,\n",
    "        cached: bool = False,\n",
    "        add_self_loops: bool = True):\n",
    "        super(A3TGCN2, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels  # 2\n",
    "        self.out_channels = out_channels # 32\n",
    "        self.periods = periods # 12\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.batch_size = batch_size\n",
    "        self._setup_layers()\n",
    "\n",
    "    def _setup_layers(self):\n",
    "        self._base_tgcn = TGCN2(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,  \n",
    "            batch_size=self.batch_size,\n",
    "            improved=self.improved,\n",
    "            cached=self.cached, \n",
    "            add_self_loops=self.add_self_loops)\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self._attention = torch.nn.Parameter(torch.empty(self.periods, device=device))\n",
    "        torch.nn.init.uniform_(self._attention)\n",
    "\n",
    "    def forward( \n",
    "        self, \n",
    "        X: torch.FloatTensor,\n",
    "        edge_index: torch.LongTensor, \n",
    "        edge_weight: torch.FloatTensor = None,\n",
    "        H: torch.FloatTensor = None\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Making a forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph. If the hidden state matrix is not present\n",
    "        when the forward pass is called it is initialized with zeros.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** (PyTorch Float Tensor): Node features for T time periods.\n",
    "            * **edge_index** (PyTorch Long Tensor): Graph edge indices.\n",
    "            * **edge_weight** (PyTorch Long Tensor, optional)*: Edge weight vector.\n",
    "            * **H** (PyTorch Float Tensor, optional): Hidden state matrix for all nodes.\n",
    "\n",
    "        Return types:\n",
    "            * **H** (PyTorch Float Tensor): Hidden state matrix for all nodes.\n",
    "        \"\"\"\n",
    "        H_accum = 0\n",
    "        probs = torch.nn.functional.softmax(self._attention, dim=0)\n",
    "        for period in range(self.periods):\n",
    "\n",
    "            H_accum = H_accum + probs[period] * self._base_tgcn( X[:, :, :, period], edge_index, edge_weight, H) #([32, 207, 32]\n",
    "\n",
    "        return H_accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1568b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yang/miniconda3/envs/pygt/lib/python3.8/site-packages/torch_geometric_temporal/nn/recurrent/temporalgcn.py'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch_geometric_temporal.nn.recurrent.temporalgcn.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857e5c1",
   "metadata": {},
   "source": [
    "class TGCN2(torch.nn.Module):\n",
    "    r\"\"\"An implementation THAT SUPPORTS BATCHES of the Temporal Graph Convolutional Gated Recurrent Cell.\n",
    "    For details see this paper: `\"T-GCN: A Temporal Graph ConvolutionalNetwork for\n",
    "    Traffic Prediction.\" <https://arxiv.org/abs/1811.05320>`_\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        batch_size (int): Size of the batch.\n",
    "        improved (bool): Stronger self loops. Default is False.\n",
    "        cached (bool): Caching the message weights. Default is False.\n",
    "        add_self_loops (bool): Adding self-loops for smoothing. Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, batch_size: int, improved: bool = False, cached: bool = False, \n",
    "                 add_self_loops: bool = True):\n",
    "        super(TGCN2, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.batch_size = batch_size\n",
    "        self._create_parameters_and_layers()\n",
    "\n",
    "    def _create_update_gate_parameters_and_layers(self):\n",
    "        self.conv_z = GCNConv(in_channels=self.in_channels,  out_channels=self.out_channels, improved=self.improved,\n",
    "                              cached=self.cached, add_self_loops=self.add_self_loops )\n",
    "        self.linear_z = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_reset_gate_parameters_and_layers(self):\n",
    "        self.conv_r = GCNConv(in_channels=self.in_channels, out_channels=self.out_channels, improved=self.improved,\n",
    "                              cached=self.cached, add_self_loops=self.add_self_loops )\n",
    "        self.linear_r = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_candidate_state_parameters_and_layers(self):\n",
    "        self.conv_h = GCNConv(in_channels=self.in_channels, out_channels=self.out_channels, improved=self.improved,\n",
    "                              cached=self.cached, add_self_loops=self.add_self_loops )\n",
    "        self.linear_h = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_parameters_and_layers(self):\n",
    "        self._create_update_gate_parameters_and_layers()\n",
    "        self._create_reset_gate_parameters_and_layers()\n",
    "        self._create_candidate_state_parameters_and_layers()\n",
    "\n",
    "    def _set_hidden_state(self, X, H):\n",
    "        if H is None:\n",
    "            H = torch.zeros(self.batch_size,X.shape[1], self.out_channels).to(X.device) #(b, 207, 32)\n",
    "        return H\n",
    "\n",
    "    def _calculate_update_gate(self, X, edge_index, edge_weight, H):\n",
    "        Z = torch.cat([self.conv_z(X, edge_index, edge_weight), H], axis=2) # (b, 207, 64)\n",
    "        Z = self.linear_z(Z) # (b, 207, 32)\n",
    "        Z = torch.sigmoid(Z)\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def _calculate_reset_gate(self, X, edge_index, edge_weight, H):\n",
    "        R = torch.cat([self.conv_r(X, edge_index, edge_weight), H], axis=2) # (b, 207, 64)\n",
    "        R = self.linear_r(R) # (b, 207, 32)\n",
    "        R = torch.sigmoid(R)\n",
    "\n",
    "        return R\n",
    "\n",
    "    def _calculate_candidate_state(self, X, edge_index, edge_weight, H, R):\n",
    "        H_tilde = torch.cat([self.conv_h(X, edge_index, edge_weight), H * R], axis=2) # (b, 207, 64)\n",
    "        H_tilde = self.linear_h(H_tilde) # (b, 207, 32)\n",
    "        H_tilde = torch.tanh(H_tilde)\n",
    "\n",
    "        return H_tilde\n",
    "\n",
    "    def _calculate_hidden_state(self, Z, H, H_tilde):\n",
    "        H = Z * H + (1 - Z) * H_tilde   # # (b, 207, 32)\n",
    "        return H\n",
    "\n",
    "    def forward(self,X: torch.FloatTensor, edge_index: torch.LongTensor, edge_weight: torch.FloatTensor = None,\n",
    "                H: torch.FloatTensor = None ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Making a forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph. If the hidden state matrix is not present\n",
    "        when the forward pass is called it is initialized with zeros.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** *(PyTorch Float Tensor)* - Node features.\n",
    "            * **edge_index** *(PyTorch Long Tensor)* - Graph edge indices.\n",
    "            * **edge_weight** *(PyTorch Long Tensor, optional)* - Edge weight vector.\n",
    "            * **H** *(PyTorch Float Tensor, optional)* - Hidden state matrix for all nodes.\n",
    "\n",
    "        Return types:\n",
    "            * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\n",
    "        \"\"\"\n",
    "        H = self._set_hidden_state(X, H)\n",
    "        Z = self._calculate_update_gate(X, edge_index, edge_weight, H)\n",
    "        R = self._calculate_reset_gate(X, edge_index, edge_weight, H)\n",
    "        H_tilde = self._calculate_candidate_state(X, edge_index, edge_weight, H, R)\n",
    "        H = self._calculate_hidden_state(Z, H, H_tilde) # (b, 207, 32)\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69c7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25bece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69426670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yang/miniconda3/envs/pygt/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_geometric.nn.conv.gcn_conv.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a286c",
   "metadata": {},
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    r\"\"\"The graph convolutional operator from the `\"Semi-supervised\n",
    "    Classification with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1609.02907>`_ paper\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{X}^{\\prime} = \\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "        \\mathbf{\\hat{D}}^{-1/2} \\mathbf{X} \\mathbf{\\Theta},\n",
    "\n",
    "    where :math:`\\mathbf{\\hat{A}} = \\mathbf{A} + \\mathbf{I}` denotes the\n",
    "    adjacency matrix with inserted self-loops and\n",
    "    :math:`\\hat{D}_{ii} = \\sum_{j=0} \\hat{A}_{ij}` its diagonal degree matrix.\n",
    "    The adjacency matrix can include other values than :obj:`1` representing\n",
    "    edge weights via the optional :obj:`edge_weight` tensor.\n",
    "\n",
    "    Its node-wise formulation is given by:\n",
    "\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}^{\\top} \\sum_{j \\in\n",
    "        \\mathcal{N}(v) \\cup \\{ i \\}} \\frac{e_{j,i}}{\\sqrt{\\hat{d}_j\n",
    "        \\hat{d}_i}} \\mathbf{x}_j\n",
    "\n",
    "    with :math:`\\hat{d}_i = 1 + \\sum_{j \\in \\mathcal{N}(i)} e_{j,i}`, where\n",
    "    :math:`e_{j,i}` denotes the edge weight from source node :obj:`j` to target\n",
    "    node :obj:`i` (default: :obj:`1.0`)\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample, or :obj:`-1` to derive\n",
    "            the size from the first input(s) to the forward method.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        improved (bool, optional): If set to :obj:`True`, the layer computes\n",
    "            :math:`\\mathbf{\\hat{A}}` as :math:`\\mathbf{A} + 2\\mathbf{I}`.\n",
    "            (default: :obj:`False`)\n",
    "        cached (bool, optional): If set to :obj:`True`, the layer will cache\n",
    "            the computation of :math:`\\mathbf{\\hat{D}}^{-1/2} \\mathbf{\\hat{A}}\n",
    "            \\mathbf{\\hat{D}}^{-1/2}` on first execution, and will use the\n",
    "            cached version for further executions.\n",
    "            This parameter should only be set to :obj:`True` in transductive\n",
    "            learning scenarios. (default: :obj:`False`)\n",
    "        add_self_loops (bool, optional): If set to :obj:`False`, will not add\n",
    "            self-loops to the input graph. (default: :obj:`True`)\n",
    "        normalize (bool, optional): Whether to add self-loops and compute\n",
    "            symmetric normalization coefficients on the fly.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "\n",
    "    Shapes:\n",
    "        - **input:**\n",
    "          node features :math:`(|\\mathcal{V}|, F_{in})`,\n",
    "          edge indices :math:`(2, |\\mathcal{E}|)`,\n",
    "          edge weights :math:`(|\\mathcal{E}|)` *(optional)*\n",
    "        - **output:** node features :math:`(|\\mathcal{V}|, F_{out})`\n",
    "    \"\"\"\n",
    "\n",
    "    _cached_edge_index: Optional[Tuple[Tensor, Tensor]]\n",
    "    _cached_adj_t: Optional[SparseTensor]\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int,\n",
    "                 improved: bool = False, cached: bool = False,\n",
    "                 add_self_loops: bool = True, normalize: bool = True,\n",
    "                 bias: bool = True, **kwargs):\n",
    "\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "\n",
    "        self.lin = Linear(in_channels, out_channels, bias=False,\n",
    "                          weight_initializer='glorot')\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "        zeros(self.bias)\n",
    "        self._cached_edge_index = None\n",
    "        self._cached_adj_t = None\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_weight: OptTensor = None) -> Tensor:\n",
    "        \"\"\"\"\"\"\n",
    "\n",
    "        if self.normalize:\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                cache = self._cached_edge_index\n",
    "                if cache is None:\n",
    "                    edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_weight, x.size(self.node_dim),\n",
    "                        self.improved, self.add_self_loops)\n",
    "                    if self.cached:\n",
    "                        self._cached_edge_index = (edge_index, edge_weight)\n",
    "                else:\n",
    "                    edge_index, edge_weight = cache[0], cache[1]\n",
    "\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                cache = self._cached_adj_t\n",
    "                if cache is None:\n",
    "                    edge_index = gcn_norm(  # yapf: disable\n",
    "                        edge_index, edge_weight, x.size(self.node_dim),\n",
    "                        self.improved, self.add_self_loops)\n",
    "                    if self.cached:\n",
    "                        self._cached_adj_t = edge_index\n",
    "                else:\n",
    "                    edge_index = cache\n",
    "\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
    "                             size=None)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_weight: OptTensor) -> Tensor:\n",
    "        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n",
    "        return matmul(adj_t, x, reduce=self.aggr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pygt]",
   "language": "python",
   "name": "conda-env-pygt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
